# -*- coding: utf-8 -*-
"""CollaborativeFiltering.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1bfFA0LQoTFOqaCxi7jQAPcGWCPVMfM7O

# Developing Collaborative Filtering Movie Recommendation System
-----

The great dataset that we use, called [MovieLens](https://grouplens.org/datasets/movielens/). This dataset contains tens of millions of movie rankings (a combination of a movie ID, a user ID, and a numeric rating).

Start with importing the [Movielens 1M dataset](https://grouplens.org/datasets/movielens/1M/).  It contains 1,000,209 anonymous ratings of approximately 3,706 movies made by 6,040 MovieLens users who joined MovieLens in 2000.

## Preparations
"""

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline
import matplotlib.pyplot as plt
import numpy as np
import os.path as op
import imp
import numpy as np

from zipfile import ZipFile
try:
    from urllib.request import urlretrieve
except ImportError:  # Python 2 compat
    from urllib import urlretrieve

# this line need to be changed if not on colab:
data_folder = '/content/'


ML_1M_URL = "http://files.grouplens.org/datasets/movielens/ml-1m.zip"
ML_1M_FILENAME = op.join(data_folder,ML_1M_URL.rsplit('/', 1)[1])
ML_1M_FOLDER = op.join(data_folder,'ml-1m')

if not op.exists(ML_1M_FILENAME):
    print('Downloading %s to %s...' % (ML_1M_URL, ML_1M_FILENAME))
    urlretrieve(ML_1M_URL, ML_1M_FILENAME)

if not op.exists(ML_1M_FOLDER):
    print('Extracting %s to %s...' % (ML_1M_FILENAME, ML_1M_FOLDER))
    ZipFile(ML_1M_FILENAME).extractall(data_folder)

"""There are four different files:

- README
- movies.dat
- ratings.dat
- users.dat

README countains the description of each dat files.

## Data analysis and formating
"""

import pandas as pd
all_ratings = pd.read_csv(op.join(ML_1M_FOLDER, 'ratings.dat'), sep='::',
                          names=["user_id", "item_id", "ratings", "timestamp"],engine='python')
all_ratings.head()

list_movies_names = []
list_item_ids = []
with open(op.join(ML_1M_FOLDER, 'movies.dat'), encoding = "ISO-8859-1") as fp:
    for line in fp:
        list_item_ids.append(line.split('::')[0])
        list_movies_names.append(line.split('::')[1])

movies_names = pd.DataFrame(list(zip(list_item_ids, list_movies_names)),
               columns =['item_id', 'item_name'])
movies_names.head()

"""add the title of the movies to the `all_ratings` data."""

movies_names['item_id']=movies_names['item_id'].astype(int)
all_ratings['item_id']=all_ratings['item_id'].astype(int)
all_ratings = all_ratings.merge(movies_names,on='item_id')

all_ratings.head()

"""The dataframe `all_ratings` contains all the raw data for our problem."""

#number of entries
len(all_ratings)

# statistics of ratings
all_ratings['ratings'].describe()

# The ratings are 1, 2, 3, 4, 5
all_ratings['ratings'].unique()

"""check the user IDs."""

all_ratings['user_id'].describe()

# number of unique users
total_user_id = len(all_ratings['user_id'].unique())
print(total_user_id)

"""the users seem to be indexed from 1 to 6040. Check that below using the following code."""

list_user_id = list(all_ratings['user_id'].unique())
list_user_id.sort()

for i,j in enumerate(list_user_id):
    if j != i+1:
        print(i,j)

"""`list_user_id` contains contiguous indices from 1 to 6040.

create a new column `user_num` to get an index from 0 to 6039 for users:
"""

all_ratings['user_num'] = all_ratings['user_id'].apply(lambda x :x-1)

all_ratings.head()

"""now look at movies."""

all_ratings['item_id'].describe()

# number of unique rated items
total_item_id = len(all_ratings['item_id'].unique())
print(total_item_id)

itemnum_2_itemid = list(all_ratings['item_id'].unique())
itemnum_2_itemid.sort()
itemid_2_itemnum = {c:i for i,c in enumerate(itemnum_2_itemid)}
all_ratings['item_num'] = all_ratings['item_id'].apply(lambda x: itemid_2_itemnum[x])

def check_ratings_num(df):
    item_num = set(df['item_num'])
    if item_num == set(range(len(item_num))):
        return True
    else:
        return False

check_ratings_num(all_ratings)

all_ratings.head()

"""### Preliminary data analysis

Find the top 10 most popular movies in the dataset
"""

all_ratings['item_name'].value_counts().nlargest(100)[50:100]

"""Plot the histogram of popularity (x-axis: # of given ratings, y-axis: # of movies with the given # of ratings)."""

plt.figure(figsize=(7,5))
_, _, bars = plt.hist(all_ratings['item_name'].value_counts(), bins=7)
plt.bar_label(bars)
plt.title('Histogram of Popularity')
plt.xlabel('# of Ratings')
plt.ylabel('# of Movies')
plt.show()

"""Plot the histogram of user activity (x-axis: # of given ratings, y-axis: # of users with the given # of ratings)."""

plt.figure(figsize=(7,5))
_, _, bars = plt.hist(all_ratings['user_num'].value_counts(), bins=7)
plt.bar_label(bars)
plt.title('Histogram of User Activity')
plt.xlabel('# of Ratings')
plt.ylabel('# of Users')
plt.show()

"""Compute the average ratings for every movie and find the top 20 highly rated movies."""

average_ratings = all_ratings.groupby(["item_name"])["ratings"].mean()
average_ratings.nlargest(20)

"""---
## Train, Validation, and Test Data

split the data in _train_, _val_ and _test_ be using a pre-defined function from [scikit-learn](http://scikit-learn.org/stable/)
"""

#Split the data into train, validation and test
from sklearn.model_selection import train_test_split

ratings_trainval, ratings_test = train_test_split(all_ratings, test_size=0.1, random_state=42)

ratings_train, ratings_val = train_test_split(ratings_trainval, test_size=0.1, random_state=42)

user_id_train = ratings_train['user_id']
item_id_train = ratings_train['item_id']
rating_train = ratings_train['ratings']

user_id_test = ratings_test['user_id']
item_id_test = ratings_test['item_id']
rating_test = ratings_test['ratings']

movies_not_train = list(set(all_ratings['item_id']) -set(item_id_train))
movies_not_train_name=set(all_ratings.loc[movies_not_train]['item_name'])
print(movies_not_train_name)

"""---
## Data Load and Batching

Use GPU if available.
"""

import torch
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

"""Define a function to return a triplet of user_num, item_num, rating from the dataframe."""

def df_2_tensor(df, device):
    # return a triplet user_num, item_num, rating from the dataframe
    user_num = np.asarray(df['user_num'])
    item_num = np.asarray(df['item_num'])
    rating = np.asarray(df['ratings'])
    return torch.from_numpy(user_num).to(device), torch.from_numpy(item_num).to(device), torch.from_numpy(rating).to(device)

train_user_num, train_item_num, train_rating = df_2_tensor(ratings_train,device)

val_user_num, val_item_num, val_rating = df_2_tensor(ratings_val,device)
test_user_num, test_item_num, test_rating = df_2_tensor(ratings_test,device)

def tensor_2_dataset(user,item,rating):
    # your code here
    # Hint: check the zip function
    return list(zip(user,item,rating))

def make_dataloader(dataset,bs,shuffle):
    # your code here
    return torch.utils.data.DataLoader(dataset,batch_size=bs,shuffle=shuffle)

train_dataset = tensor_2_dataset(train_user_num,train_item_num, train_rating)
val_dataset = tensor_2_dataset(val_user_num,val_item_num,val_rating)
test_dataset = tensor_2_dataset(test_user_num, test_item_num, test_rating)

train_dataloader = make_dataloader(train_dataset,1024,True)
val_dataloader = make_dataloader(val_dataset,1024, False)
test_dataloader = make_dataloader(test_dataset,1024,False)

"""---
## The model


"""

import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim

class ScaledEmbedding(nn.Embedding):
    """
    Embedding layer that initialises its values
    to using a normal variable scaled by the inverse
    of the embedding dimension.
    """
    def reset_parameters(self):
        """
        Initialize parameters.
        """

        self.weight.data.normal_(0, 1.0 / self.embedding_dim)
        if self.padding_idx is not None:
            self.weight.data[self.padding_idx].fill_(0.0)


class ZeroEmbedding(nn.Embedding):
    """
    Used for biases.
    """

    def reset_parameters(self):
        """
        Initialize parameters.
        """

        self.weight.data.zero_()
        if self.padding_idx is not None:
            self.weight.data[self.padding_idx].fill_(0.0)

"""

```
# 此内容为代码格式
```

### Creating Own Embedding Module

Define the `Model_dot`(class to represents users and items. It's composed of a 4 `embedding` layers:

- a `(num_users x latent_dim)` embedding layer to represent users,
- a `(num_items x latent_dim)` embedding layer to represent items,
- a `(num_users x 1)` embedding layer to represent user biases, and
- a `(num_items x 1)` embedding layer to represent item biases.





"""

class DotModel(nn.Module):

    def __init__(self,
                 num_users,
                 num_items,
                 embedding_dim=32):

        super(DotModel, self).__init__()

        self.embedding_dim = embedding_dim

        self.user_embeddings = ScaledEmbedding(num_users, embedding_dim)
        self.item_embeddings = ScaledEmbedding(num_items, embedding_dim)


        self.user_biases = ZeroEmbedding(num_users, 1)
        self.item_biases = ZeroEmbedding(num_items, 1)


    def forward(self, user_ids, item_ids):

        user_embedding = self.user_embeddings(user_ids)
        item_embedding = self.item_embeddings(item_ids)

        user_bias = self.user_biases(user_ids).squeeze()
        item_bias = self.item_biases(item_ids).squeeze()

        dot = torch.mul(user_embedding, item_embedding).sum(1)
        res = dot + user_bias + item_bias

        return res

net = DotModel(total_user_id,total_item_id).to(device)

net

"""Check the network by taking  a batch from train loader."""

batch_user, batch_item, batch_rating = next(iter(train_dataloader))
batch_user, batch_item, batch_rating = batch_user.to(device), batch_item.to(device), batch_rating.to(device)

predictions = net(batch_user, batch_item)
predictions.shape

"""Use MSE loss defined below:"""

def regression_loss(predicted_ratings, observed_ratings):
    return ((observed_ratings - predicted_ratings) ** 2).mean()

loss=regression_loss(predictions,batch_rating)

loss

"""## Train and test the model"""

class FactorizationModel(object):

    def __init__(self, embedding_dim=32, n_iter=10, l2=0.0,
                 learning_rate=1e-2, device=device, net=None, num_users=None,
                 num_items=None,random_state=None):

        self._embedding_dim = embedding_dim
        self._n_iter = n_iter
        self._learning_rate = learning_rate
        self._l2 = l2
        self._device = device
        self._num_users = num_users
        self._num_items = num_items
        self._net = net
        self._optimizer = None
        self._loss_func = None
        self._random_state = random_state or np.random.RandomState()


    def _initialize(self):
        if self._net is None:
            self._net = DotModel(self._num_users, self._num_items, self._embedding_dim).to(self._device)

        self._optimizer = optim.Adam(
                self._net.parameters(),
                lr=self._learning_rate,
                weight_decay=self._l2
            )

        self._loss_func = regression_loss

    @property
    def _initialized(self):
        return self._optimizer is not None


    def fit(self, dataloader, val_dataloader, verbose=True):
        if not self._initialized:
            self._initialize()

        valid_loss_min = np.Inf # track change in validation loss
        train_losses, valid_losses, valid_maes =[], [], []

        for epoch_num in range(self._n_iter):
            tot_train_loss = 0.0
            ###################
            # train the model #
            ###################

            self._net.train()
            for batch_user, batch_item, batch_rating in dataloader:

                bath_user=batch_user.to(device)
                batch_item=batch_item.to(device)
                batch_rating=batch_rating.to(device)

                #1. Compute the output
                predictions = self._net(batch_user,batch_item)
                #2. Compute the loss
                #loss = self._loss_func(predictions,batch_rating)
                loss = self._loss_func(predictions,batch_rating)
                # 3. Zero out the gradient
                self._optimizer.zero_grad()
                # 4. Backward pass, compute the new gradients
                loss.backward()
                # 5. Update the weights
                self._optimizer.step()

                tot_train_loss += loss.item()


            train_loss = tot_train_loss /len(dataloader)
            # Go to the validation loop
            valid_loss, valid_mae = self.test(val_dataloader)
            train_losses.append(train_loss)
            valid_losses.append(valid_loss)
            valid_maes.append(valid_mae)

            if verbose:
                print('Epoch {}: loss_train {}, loss_val {}'.format(epoch_num, train_loss,valid_loss))

            if np.isnan(train_loss) or train_loss == 0.0:
                raise ValueError('Degenerate train loss: {}'.format(train_loss))

            if valid_loss <= valid_loss_min:
              print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(valid_loss_min,valid_loss))
              torch.save(self._net.state_dict(), 'model_cf.pt')
              valid_loss_min = valid_loss

        return train_losses, valid_losses, valid_maes


    ######################
    # validate/Test the model #
    ######################
    def test(self,dataloader, verbose = False):
        self._net.eval()
        L1loss = torch.nn.L1Loss()
        tot_test_loss = 0.0
        tot_test_mae = 0.0

        with torch.no_grad():
          for batch_user, batch_item, batch_rating in dataloader:
            #1. compute the output
            predictions = self._net(batch_user,batch_item)
            #2. Compute the loss
            loss = self._loss_func(predictions,batch_rating)

            tot_test_loss += loss.item()
            tot_test_mae += L1loss(predictions,batch_rating.type(torch.FloatTensor).to(device))


        test_loss = tot_test_loss / len(dataloader)
        test_mae = tot_test_mae / len(dataloader)
        if verbose:
            print(f"RMSE: {np.sqrt(test_loss)}, MAE: {test_mae}")
        return test_loss, test_mae

model = FactorizationModel(embedding_dim=50,  # latent dimensionality
                                   n_iter=40,  # number of epochs of training
                                   learning_rate=5e-4,
                                   l2=1e-8,  # strength of L2 regularization
                                   num_users=total_user_id,
                                   num_items=total_item_id)

train_losses,valid_losses,valid_maes =model.fit(train_dataloader,val_dataloader)

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline
# %config InlineBackend.figure_format = 'retina'

import matplotlib.pyplot as plt

plt.plot(np.log(train_losses), label='Training loss')
plt.plot(np.log(valid_losses), label='Validation loss')
plt.legend(frameon=False)

"""Compute the RMSE and mean absolute error on the test data."""

model._net.load_state_dict(torch.load('model_cf.pt'))

test_loss, test_mae = model.test(test_dataloader,True)

"""our RMSE is around 0.85 and MAE is 0.67

## Optimize your model and training
"""

model_tuning = FactorizationModel(embedding_dim=128, # embedding_dim 50 --> 128
                  n_iter=20,
                  learning_rate=5e-4,
                  l2=5e-8,  # l2-penalty 1e-8 --> 5e-8
                  num_users=total_user_id,
                  num_items=total_item_id)
train_losses,test_losses, test_maes = model_tuning.fit(train_dataloader,val_dataloader)

model_tuning._net.load_state_dict(torch.load('model_cf_tuned.pt'))
test_loss, test_mae = model_tuning.test(test_dataloader,verbose=True)

"""## Interpreting Biases


"""

item_bias_np = model_tuning._net.item_biases.weight.data.cpu().numpy()
item_bias_np = item_bias_np.squeeze()

# construct a dictionary that maps item_num to item_name, and vice versa
numitem_2_name = {i:name for name,i in np.asarray(all_ratings[['item_name', 'item_num']])}
name_2_numitem = {name:i for name,i in np.asarray(all_ratings[['item_name', 'item_num']])}

# Construct a list of movie names and the corresponding bias.
list_name_bias = [[name, item_bias_np[name_2_numitem[name]]] for name in list(ratings_train['item_name'].unique())]

list_name_bias.sort(key= lambda x: x[1])
list_name_bias[-10:]

"""## PCA of movies' embeddings

"""

from sklearn.decomposition import PCA
from operator import itemgetter

item_emb_np = model_tuning._net.item_embeddings.weight.data.cpu().numpy()

#perform PCA to extract the 4 principal components
pca = PCA(n_components=4)
latent_fac = pca.fit_transform(item_emb_np)

#Here we get the top 1000 mostly rated movies
g = all_ratings.groupby('item_name')['ratings'].count()
most_rated_movies = g.sort_values(ascending=False).index.values[:1000]
# we get the corresponding movie numbers
most_rated_movies_num = [name_2_numitem[n] for n in most_rated_movies]

nums = most_rated_movies_num[:80]
txt_movies_names = [numitem_2_name[i] for i in nums]
X = latent_fac[nums,1]
Y = latent_fac[nums,2]
plt.figure(figsize=(15,15))
plt.scatter(X, Y)
for i, x, y in zip(txt_movies_names, X, Y):
    plt.text(x+0.01,y-0.01,i, fontsize=11)
plt.show()

"""## Predict our own ratings


"""

my_ratings = {'American Beauty (1999)' : 5,
        'Star Wars: Episode IV - A New Hope (1977)' : 2,
        'Star Wars: Episode V - The Empire Strikes Back (1980)' : 3,
        'Star Wars: Episode VI - Return of the Jedi (1983)' : 2,
        'Jurassic Park (1993)' : 4,
        'Saving Private Ryan (1998)' : 4,
        'Terminator 2: Judgment Day (1991)' : 3,
        'Matrix, The (1999)' : 4,
        'Back to the Future (1985)' : 5,
        'Silence of the Lambs, The (1991)' : 5,
        'Men in Black (1997)' : 3,
        'Raiders of the Lost Ark (1981)' : 2,
        'Fargo (1996)' : 3,
        'Sixth Sense, The (1999)' : 2,
        'Braveheart (1995)' : 5,
        'Shakespeare in Love (1998)' : 5,
        'Princess Bride, The (1987)' : 3,
        "Schindler's List (1993)" : 5,
        'L.A. Confidential (1997)' : 2,
        'Groundhog Day (1993)' : 2,
        'E.T. the Extra-Terrestrial (1982)' : 3,
        'Star Wars: Episode I - The Phantom Menace (1999)' : 3,
        'Being John Malkovich (1999)' : 5,
        'Shawshank Redemption, The (1994)' : 5,
        'Godfather, The (1972)' : 5,
        'Forrest Gump (1994)' : 5,
        'Ghostbusters (1984)' : 3,
        'Pulp Fiction (1994)' : 3,
        'Terminator, The (1984)' : 3,
        'Toy Story (1995)' : 3,
        'Alien (1979)' : 1,
        'Total Recall (1990)' : 5,
        'Fugitive, The (1993)' : 4,
        'Gladiator (2000)' : 3,
        'Aliens (1986)' : 3,
        'Blade Runner (1982)' : 1,
        'Who Framed Roger Rabbit? (1988)' : 3,
        'Stand by Me (1986)' : 4,
        'Usual Suspects, The (1995)' : 5,
        'Babe (1995)' : 3}

my_rated_items = [name_2_numitem[name] for name in list(my_ratings.keys())]

# Ridge regression

from sklearn.linear_model import Ridge
clf = Ridge(alpha=1, fit_intercept=True)
clf.fit(item_emb_np[my_rated_items, :], list(my_ratings.values()))
my_emb_np = clf.coef_
my_emb_bias = clf.intercept_

# compute pred_results
# regression coef_ as my_emb_np, regression intercept_ as bias

my_rating_results = (my_emb_np * item_emb_np).sum(1) + item_bias_np + my_emb_bias
my_results = [(numitem_2_name[item], ratings) for item, ratings in enumerate(my_rating_results)]
my_results_df = pd.DataFrame(my_results, columns = ["item_name", "pred_ratings"])

# check: pred_ratings on rated movies V.S. input_ratings
# columns "pred_ratings", "input_ratings" should be consistent

my_rating_df = my_results_df[my_results_df['item_name'].isin(list(my_ratings.keys()))]
my_rating_df["input_ratings"] = my_rating_df["item_name"].apply(lambda name: my_ratings[name])
my_rating_df

my_pred_df = my_results_df[~my_results_df['item_name'].isin(list(my_ratings.keys()))]
my_pred_df.nlargest(10, columns='pred_ratings')

"""---
## Movie valuation


- For each user, the total amount of viewing time is proportional to the number of observed ratings. In other words, more active users spend more time on Netflix;

- For each given user, the viewing time across different movies follow a [Zipf's law](https://en.wikipedia.org/wiki/Zipf%27s_law), that is

  $$
    \text{Viewtime of movie $i$} \propto \frac{1}{\text{rank of } i }
  $$
  Here the rank of $i$ is determined by sorting the **predicted rating** of movie $i$ by the user in descending order. In other words, the user spend less time on watching movies with lower ratings.

- Netflix has a fixed total budget of 1 Billion dollars to license the content;

- Netflix estimates the value of a movie based on the expected share of customer viewing time of the movie on its platform:
$$
\text{Value of a movie $i$ } = \frac{\text{View time of movie $i$ of all users}}{\text{Total view time of all users} } \times \text{Total content license budget}
$$

### Compute the value of each movie
"""

# Get indices for top 1000 active users and top 1000 mostly rated movies
top1000_user_num = pd.Series(all_ratings["user_num"].value_counts()[:1000].index, name="user_num")
top1000_item_num = pd.Series(all_ratings["item_num"].value_counts()[:1000].index, name="item_num")
# Construct dataframes for storing predicted ratings
pred_ratings = pd.merge(top1000_user_num, top1000_item_num, how="cross")
cartesian_user_num = torch.from_numpy(np.asarray(pred_ratings["user_num"])).to(device)
cartesian_item_num = torch.from_numpy(np.asarray(pred_ratings["item_num"])).to(device)

pred_ratings.head()

# sanity check: # of rows in pred_ratings == # of active users (1000) x # of popular items (1000)
pred_ratings.shape[0] == 1000 * 1000

# model_tuning._net predictions over cartesian_user_num and cartesian_item_num
model_tuning._net.eval()
pred_ratings["pred_ratings"] = model_tuning._net(cartesian_user_num, cartesian_item_num).cpu().detach().numpy()

# Each user's total viewtime is stored in Series user_total_viewtime
user_total_viewtime = all_ratings["user_num"].value_counts()[:1000] / all_ratings["user_num"].value_counts()[:1000].sum()
user_total_viewtime.name = "user_total_viewtime"
user_total_viewtime.index.name = "user_num"
pred_ratings = pd.merge(pred_ratings, user_total_viewtime, left_on="user_num", right_on="user_num", how="left")

pred_ratings.head()

# Each user per each movie viewtime: pred_all_ratings["user_item_viewtime"] = ["1/user_rank"] / ["sum(1/user_rank)"] * ["user_total_viewtime"]
# these wordy codes are to reduce RAM consumption otherwise Colab may crush
pred_ratings["1/user_rank"] = 1/pred_ratings.groupby("user_num")["pred_ratings"].rank(method = "min", ascending = False)
sum_inverse_user_rank = pred_ratings.groupby("user_num")["1/user_rank"].sum()
sum_inverse_user_rank.name = "sum(1/user_rank)"
pred_ratings = pd.merge(pred_ratings, sum_inverse_user_rank, left_on="user_num", right_on="user_num", how="left")
pred_ratings["user_item_viewtime"] = pred_ratings["1/user_rank"] / pred_ratings["sum(1/user_rank)"] * pred_ratings["user_total_viewtime"]

pred_ratings

# sanity check: sum of user_item_viewtime == 1
pred_ratings["user_item_viewtime"].sum()

# compute each movie's value
budget =  1000000000
movie_values = pred_ratings.groupby("item_num")["user_item_viewtime"].sum() * budget
movie_values.name = "item_value"

# replace item_num by item_name
movie_values = movie_values.to_frame()
movie_values["item_name"] = [numitem_2_name[item_num] for item_num in movie_values.index]
movie_values = movie_values.set_index("item_name").squeeze(axis=1)

movie_values.head()

# sanity check: sum of movie values == budget
movie_values.sum() == budget

"""### Output the top valued movies"""

movie_values.nlargest(30)

movie_values['Toy Story (1995)']

"""
### Compare the output with the mostly rated movies. Identify the discrepancies."""

highest_values_30 = movie_values.nlargest(30).index
most_rated_30 = all_ratings['item_name'].value_counts().nlargest(30).index

# movies IN top 30 valued AND IN most 30 rated
set(highest_values_30).intersection(set(most_rated_30))

# movies IN top 30 valued but NOT in most 30 rated
set(highest_values_30) - set(most_rated_30)

# movies IN most 30 rated but NOT IN top 30 valued
set(most_rated_30) - set(highest_values_30)

pred_ratings.groupby("item_num")['pred_ratings'].mean()[name_2_numitem['Toy Story (1995)']]

all_ratings['item_name'].value_counts()['Toy Story (1995)']

all_ratings['item_name'].value_counts()['Seven Samurai (The Magnificent Seven) (Shichinin no samurai) (1954)']

pred_ratings.groupby("item_num")['pred_ratings'].mean()[name_2_numitem['Seven Samurai (The Magnificent Seven) (Shichinin no samurai) (1954)']]